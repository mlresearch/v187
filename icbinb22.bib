@proceedings{icbinb-2022,
  booktitle =	 {Proceedings on "I Can't Believe It's Not Better!  -
                  Understanding Deep Learning Through Empirical
                  Falsification" at NeurIPS 2022 Workshops},
  editor =	 {Antor\'an, Javier and Blaas, Arno and Feng, Fan and
                  Ghalebikesabi, Sahra and Mason,Ian and Pradier,
                  Melanie F. and Rohde, David and Ruiz, Francisco
                  J. R. and Schein, Aaron },
  year =	 2022,
  shortname =	 {ICBINB 22},
  volume =	 187,
  start =	 {2022-12-03},
  end =		 {2022-12-03},
  published =	 {2023-02-28},
  address =	 {New Orleans, Louisiana, USA},
  conference_url ={https://sites.google.com/view/icbinb-2022/},
  conference_number =3,
}

@InProceedings{saul22,
  title =	 {Lempel-Ziv Networks },
  author =	 {Saul, Rebecca and Alam, Mohammad Mahmudul and
                  Hurwitz, John and Raff, Edward and Oates, Tim and
                  Holt, James },
  pages =	 {1-11},
  abstract =	 {Sequence processing has long been a central area of
                  machine learning research. Recurrent neural nets
                  have been successful in processing sequences for a
                  number of tasks; however, they are known to be both
                  ineffective and computationally expensive when
                  applied to very long sequences. Compression-based
                  methods have demonstrated more robustness when
                  processing such sequences --- in particular, an
                  approach pairing the Lempel-Ziv Jaccard Distance
                  (LZJD) with the k-Nearest Neighbor algorithm has
                  shown promise on long sequence problems (up to
                  steps) involving malware
                  classification. Unfortunately, use of LZJD is
                  limited to discrete domains. To extend the benefits
                  of LZJD to a continuous domain, we investigate the
                  effectiveness of a deep-learning analog of the
                  algorithm, the Lempel-Ziv Network. While we achieve
                  successful proof-of-concept, we are unable to
                  meaningfully improve on the performance of a
                  standard LSTM across a variety of datasets and
                  sequence processing tasks. In addition to presenting
                  this negative result, our work highlights the
                  problem of sub-par baseline tuning in newer research
                  areas.},
}

@InProceedings{zaidi22,
  title =	 {When Does Re-initialization Work?},
  author =	 {Zaidi, Sheheryar and Berariu, Tudor and Kim, Hyunjik
                  and Bornschein, Jorg and Clopath, Claudia and Teh,
                  Yee Whye and Pascanu, Razvan },
  pages =	 {12-26},
  abstract =	 {Re-initializing a neural network during training has
                  been observed to improve generalization in recent
                  works. Yet it is neither widely adopted in deep
                  learning practice nor is it often used in
                  state-of-the-art training protocols. This raises the
                  question of when re-initialization works, and
                  whether it should be used together with
                  regularization techniques such as data augmentation,
                  weight decay and learning rate schedules. In this
                  work, we conduct an extensive empirical comparison
                  of standard training with a selection of
                  re-initialization methods to answer this question,
                  training over 15,000 models on a variety of image
                  classification benchmarks. We first establish that
                  such methods are consistently beneficial for
                  generalization in the absence of any other
                  regularization. However, when deployed alongside
                  other carefully tuned regularization techniques,
                  re-initialization methods offer little to no added
                  benefit for generalization, although optimal
                  generalization performance becomes less sensitive to
                  the choice of learning rate and weight decay
                  hyperparameters. To investigate the impact of
                  re-initialization methods on noisy data, we also
                  consider learning under label noise. Surprisingly,
                  in this case, re-initialization significantly
                  improves upon standard training, even in the
                  presence of other carefully tuned regularization
                  techniques.},
}

@InProceedings{oldewage22,
  title =	 {Adversarial Attacks are a Surprisingly Strong
                  Baseline for Poisoning Few-Shot Meta-Learners},
  author =	 {Oldewage, Elre T. and Bronskill, John and Turner,
                  Richard E. },
  pages =	 {27-40},
  abstract =	 {This paper examines the robustness of deployed
                  few-shot meta-learning systems when they are fed an
                  imperceptibly perturbed few-shot dataset. We attack
                  amortized meta-learners, which allows us to craft
                  colluding sets of inputs that are tailored to fool
                  the system's learning algorithm when used as
                  training data. Jointly crafted adversarial inputs
                  might be expected to synergistically manipulate a
                  classifier, allowing for very strong data-poisoning
                  attacks that would be hard to detect. We show that
                  in a white box setting, these attacks are very
                  successful and can cause the target model's
                  predictions to become worse than chance. However, in
                  opposition to the well-known transferability of
                  adversarial examples in general, the colluding sets
                  do not transfer well to different classifiers. We
                  explore two hypotheses to explain this:
                  'overfitting' by the attack, and mismatch between
                  the model on which the attack is generated and that
                  to which the attack is transferred. Regardless of
                  the mitigation strategies suggested by these
                  hypotheses, the colluding inputs transfer no better
                  than adversarial inputs that are generated
                  independently in the usual way.},
}

@InProceedings{loaiza22,
  title =	 {Denoising Deep Generative Models },
  author =	 {Loaiza-Ganem, Gabriel and Ross, Brendan Leigh and
                  Wu, Luhuan and Cunningham, John Patrick and
                  Cresswell, Jesse C. and Caterini, Anthony L. },
  pages =	 {41-50},
  abstract =	 {Likelihood-based deep generative models have
                  recently been shown to exhibit pathological
                  behaviour under the manifold hypothesis as a
                  consequence of using high-dimensional densities to
                  model data with low-dimensional structure. In this
                  paper we propose two methodologies aimed at
                  addressing this problem. Both are based on adding
                  Gaussian noise to the data to remove the
                  dimensionality mismatch during training, and both
                  provide a denoising mechanism whose goal is to
                  sample from the model as though no noise had been
                  added to the data. Our first approach is based on
                  Tweedie's formula, and the second on models which
                  take the variance of added noise as a conditional
                  input. We show that surprisingly, while well
                  motivated, these approaches only sporadically
                  improve performance over not adding noise, and that
                  other methods of addressing the dimensionality
                  mismatch are more empirically adequate.},
}

@InProceedings{simran22,
  title =	 {On the Maximum Hessian Eigenvalue and
                  Generalization},
  author =	 {Kaur, Simran and Cohen, Jeremy and Lipton, Zachary
                  Chase},
  pages =	 {51-65},
  abstract =	 {The mechanisms by which certain training
                  interventions, such as increasing learning rates and
                  applying batch normalization, improve the
                  generalization of deep networks remains a
                  mystery. Prior works have speculated that "flatter"
                  solutions generalize better than "sharper" solutions
                  to unseen data, motivating several metrics for
                  measuring flatness (particularly $\lambda_{\rm max}$
                  , the largest eigenvalue of the Hessian of the
                  loss); and algorithms, such as Sharpness-Aware
                  Minimization (SAM), that directly optimize for
                  flatness. Other works question the link between
                  $\lambda_{\rm max}$ and generalization. In this
                  paper, we present findings that call $\lambda_{\rm
                  max}$'s influence on generalization further into
                  question. We show that: (1) while larger learning
                  rates reduce $\lambda_{\rm max}$ for all batch
                  sizes, generalization benefits sometimes vanish at
                  larger batch sizes; (2) by scaling batch size and
                  learning rate simultaneously, we can change
                  $\lambda_{\rm max}$ without affecting
                  generalization; (3) while SAM produces smaller
                  $\lambda_{\rm max}$ for all batch sizes,
                  generalization benefits (also) vanish with larger
                  batch sizes; (4) for dropout, excessively high
                  dropout probabilities can degrade generalization,
                  even as they promote smaller $\lambda_{\rm max}$ ;
                  and (5) while batch-normalization does not
                  consistently produce smaller $\lambda_{\rm max}$ ,
                  it nevertheless confers generalization
                  benefits. While our experiments affirm the
                  generalization benefits of large learning rates and
                  SAM for minibatch SGD, the GD-SGD discrepancy
                  demonstrates limits to $\lambda_{\rm max}$'s ability
                  to explain generalization in neural networks.},
}

@InProceedings{likhomanenko22,
  title =	 {Continuous Soft Pseudo-Labeling in ASR },
  author =	 {Likhomanenko, Tatiana and Collobert, Ronan and
                  Jaitly, Navdeep and Bengio, Samy },
  pages =	 {66-84},
  abstract =	 {Continuous pseudo-labeling (PL) algorithms such as
                  slimIPL have recently emerged as a powerful strategy
                  for semi-supervised learning in speech
                  recognition. In contrast with earlier strategies
                  that alternated between training a model and
                  generating pseudo-labels (PLs) with it, here PLs are
                  generated in end-to-end manner as training proceeds,
                  improving training speed and the accuracy of the
                  final model.  PL shares a common theme with
                  teacher-student models such as distillation in that
                  a teacher model generates targets that need to be
                  mimicked by the student model being
                  trained. However, interestingly, PL strategies in
                  general use hard-labels, whereas distillation uses
                  the distribution over labels as the target to mimic.
                  Inspired by distillation we expect that specifying
                  the whole distribution (aka soft-labels) over
                  sequences as the target for unlabeled data, instead
                  of a single best pass pseudo-labeled transcript
                  (hard-labels) should improve PL performance and
                  convergence. Surprisingly and unexpectedly, we find
                  that soft-labels targets can lead to training
                  divergence, with the model collapsing to a
                  degenerate token distribution per frame. We
                  hypothesize that the reason this does not happen
                  with hard-labels is that training loss on
                  hard-labels imposes sequence-level consistency that
                  keeps the model from collapsing to the degenerate
                  solution.  In this paper, we show several
                  experiments that support this hypothesis, and
                  experiment with several regularization approaches
                  that can ameliorate the degenerate collapse when
                  using soft-labels. These approaches can bring the
                  accuracy of soft-labels closer to that of
                  hard-labels, and while they are unable to outperform
                  them yet, they serve as a useful framework for
                  further improvements.},
}

@InProceedings{moturu22,
  title =	 {Volume-based Performance not Guaranteed by Promising
                  Patch-based Results in Medical Imaging },
  author =	 {Abhishek Moturu and Sayali Joshi and Andrea S. Doria
                  and Anna Goldenberg},
  pages =	 {85-93},
  abstract =	 {Whole-body MRIs are commonly used to screen for
                  early signs of cancer. In addition to the small size
                  of tumours at onset, variations in individuals,
                  tumour types, and MRI machines increase the
                  difficulty of finding tumours in these scans. Using
                  patches, rather than whole-body scans, to train a
                  deep-learning-based segmentation model with a custom
                  compound patch loss function, several augmentations,
                  and additional synthetically generated training data
                  to identify areas where there is a high probability
                  of a tumour provided promising results at the
                  patch-level. However, applying the patch-based model
                  to the entire volume did not yield great results
                  despite all of the state-of-the-art improvements,
                  with over 50\% of the tumour sections in the dataset
                  being missed. Our work highlights the discrepancy
                  between the commonly used patch-based analysis and
                  the overall performance on the whole image and the
                  importance of focusing on the metrics relevant to
                  the ultimate user in our case, the clinician. Much
                  work remains to be done to bring state-of-the-art
                  segmentation to the clinical practice of cancer
                  screening.},
}
